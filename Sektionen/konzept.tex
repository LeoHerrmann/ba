In diesem Kapitel wird erklärt, wie eine Messumgebung aufgesetzt wurde, um die Robustheit
%und Generalisierbarkeit
der Lösungsverfahren empirisch zu bewerten. In dieser Messumgebung spielen zwei Agents, die die zu untersuchenden Ansätze implementieren, das Spiel wiederholt gegeneinander. Dabei werden deren Gewinnraten und die Spieldauer gemessen. Da bei Vier Gewinnt, wie in Kapitel \ref{vier-gewinnt} erwähnt, der Spieler, der den ersten Stein platziert, einen Vorteil hat, wechselt bei jeder Wiederholung das Recht, den ersten Zug zu machen. Diese Messungen werden unter verschiedenen Szenarien durchgeführt, die jeweils auf ein in Kapitel \ref{robustheit}. definierten Aspekt der Robustheit abzielt:

\begin{itemize}
	\item Unsicherheit bezüglich Aktionen: Mit einer bestimmten Wahrscheinlichkeit landet ein Spielstein nicht in der vorgesehenen Spalte sondern in einer benachbarten Spalte.
	\item Unsicherheit bezüglich Beobachtungen: Agents erhalten fehlerhafte Informationen über das Spielfeld. Es ist anzumerken, dass hierbei aus dem MDP ein Partially Observable MDP (POMDP) wird, also ein MDP dessen Umgebung nur teilweise oder fehlerhaft beobachtbar ist. Der betroffene Agent kann nicht mit Sicherheit gesagt werden, in welchem Zustand er sich gerade befindet. Zusätzlich zum MDP enthält das POMDP ein Observation Modell O(s, o), das die Wahrscheinlichkeit beschreibt, eine Beobachtung o im Zustand s zu machen. POMDPs sind wesentlich komplizierter gezielt zu lösen, in der realen Welt jedoch wesentlich häufiger anzutreffen. Es gibt Optimierungen von MCTS und bestimmte RL-Methoden, über die POMDPs gezielt gelöst werden können (\cite{Russell.2020}, S. 588 ff.). Da es in dieser Arbeit darum geht, zu untersuchen, inwiefern grundsätzliche Eigenschaften von symbolischen Algorithmen und Reinforcement Learning Robustheit beeinflussen, werden diese gezielten Lösungen zur Vereinfachung nicht betrachtet. Beide Agenten gehen davon aus, dass es sich bei dem verrauschten Bild um den tatsächlichen Zustand des Spiels handelt.
	\item Unsicherheit bezüglich Dynamik der Umgebung: Mit einer bestimmten Wahrscheinlichkeit führt der Gegenspieler nicht den Zug aus, den er für am besten hält, sondern einen zufälligen Zug.
	\item Unsicherheit bezüglich Dynamik der Umgebung: Mit einer bestimmten Wahrscheinlichkeit führt der Gegenspieler mehrere Züge hintereinander durch.
	% \item Generalisierbarkeit: Zum Gewinnen werden nicht vier Spielsteine in einer Reihe benötigt, sondern fünf.
\end{itemize}

Darüber hinaus werden die Messungen in einer neutralen Umgebung durchgeführt, um eine Grundlage für die Interpretation der Ergebnisse zu bilden.

Als Grundlage für die Messumgebung dient das PettingZoo-Toolkit. Es abstrahiert Probleme in Umgebungen und stellt eine Schnittstelle für Agents bereit, die mit verschiedene Lösungsstrategien mit den Umgebungen interagieren. Eine Umgebung, die das Spiel Vier Gewinnt abstrahiert, ist Teil des PettingZoo Toolkits. Es kommen Reinforcement Learning Modelle zum Einsatz, die aus RL-Bibliotheken wie CleanRL oder Stable-Baselines bereitgestellt werden. Falls vorhanden, wird auf fertig implementierte Algorithmen zurückgegriffen.