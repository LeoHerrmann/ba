This thesis investigates the relative robustness of symbolic algorithms versus reinforcement learning methods, using the board game Connect Four as a case study. Two agents were implemented that play the game autonomously. The first agent utilizes the symbolic algorithm Monte Carlo Tree Search (MCTS), while the second agent is based on the reinforcement learning method Proximal Policy Optimization (PPO). The robustness of the agents is quantified by measuring the decrease in win rate against an agent that plays randomly, while the agents under investigation are subjected to two different scenarios with unfavorable conditions that target various aspects of robustness. In the first scenario, there is uncertainty regarding actions, meaning that the agents do not have complete control over the column in which they place their pieces. In the second scenario, the agents receive erroneous information about the game board, thereby introducing uncertainty regarding observations.

It is shown that, among the two implemented agents, the MCTS agent is more robust than the PPO agent in the scenario involving uncertainty regarding actions. For the scenario with uncertainty regarding observations, this was also observed up to certain levels of uncertainty. However, the results of this study cannot be generalized to a comparison of robustness between MCTS and PPO independent of the specific implementation or application, as the implemented PPO agent was insufficiently trained for a fair comparison. Further investigations are necessary to draw general conclusions regarding the comparison of robustness between symbolic algorithms and reinforcement learning.

\newpage