\label{konzept}

In diesem Kapitel wird erläutert, wie die Fragestellung dieser Arbeit beantwortet werden soll, inwiefern symbolische Algorithmen oder RL-Verfahren robuster sind.

Dies geschieht exemplarisch am Beispiel von Vier Gewinnt, da dieses Spiel eine kontrollierbare Umgebung darstellt, die reproduzierbare und vergleichbare Messungen ermöglicht. Außerdem wurde in verschiedenen Studien gezeigt, dass sich sowohl symbolische als auch RL-basierte Lösungsverfahren zur Lösung dieses Spiels eigenen. Als symbolischer Algorithmus wird hier MCTS eingesetzt, da dieses Verfahren unabhängig von problemspezifischen Heuristiken angewandt werden kann, und im Gegensatz zum Minimax-Algorithmus und Alpha-Beta-Pruning auch bei komplexeren Problemen mit einer akzeptablen Laufzeit betrieben werden kann. RL-Verfahren werden in dieser Arbeit durch PPO vertreten. Dabei handelt es sich um eine Optimierung des Verfahrens A2C. Genau wie A2C handelt es sich um ein approximierendes, modellfreies on-policy Verfahren, welche sich wie in Kapitel \ref{reinforcement-learning} herausgearbeitet, zur Lösung von Vier Gewinnt am besten eigenen, ohne dabei auf spezielle Verfahren für Multi-Agent-Probleme zurückzugreifen. Im Vergleich zu A2C erleichtert PPO durch seine geringere Sensibilität gegenüber Änderungen an Hyperparametern die Implementierung.

Die Robustheit der Verfahren wird bewertet, indem Agenten, die diese Verfahren implementieren, dazu veranlasst werden, das Spiel wiederholt gegen einen zufällig spielenden Agenten zu spielen. Dabei werden die Agenten unter bestimmten Szenarien beeinträchtigt, welche auf die in Kapitel \ref{robustheit} genannten Aspekte von Robustheit abzielen und es wird gemessen und verglichen, wie stark sich die Beeinträchtigungen auf die Gewinnrate auswirken. Es wäre auch denkbar, den MCTS-Agenten und den PPO-Agenten im Spiel gegeneinander zu untersuchen, während einer der beiden Agenten oder beide Agenten den Beeinträchtigungen ausgesetzt sind. Dies hat allerdings den Nachteil, dass im Anschluss nicht klar herausgearbeitet werden kann, wie sehr ein Agent durch ein Szenario beeinträchtigt wird oder sein geschwächtes Leistungsvermögen durch den Gegenspieler ausgenutzt wird. Um die Vergleichbarkeit und Unabhängigkeit der Ergebnisse zu gewährleisten, spielen die beiden Agenten daher gegen einen zufällig spielenden Agenten.

Die Reduktion der Gewinnrate eines Agenten in einem Szenario mit Beeinträchtigung wird dabei relativ zur Gewinnrate im neutralen Szenario ohne Beeinträchtigungen betrachtet, um einen Ausgleich dafür zu schaffen, dass die Agenten im neutralen Szenario nicht dieselbe Gewinnrate gegen den zufällig spielenden Agenten erzielen und die Gewinnrate eines Agenten, der im neutralen Szenario eine niedrigere Gewinnrate erzielt, auch weniger weit fallen kann. Die Betrachtung erfolgt außerdem relativ zur Gewinnrate von 50 \%, da ein Agent ab dem Punkt, ab dem er eine Gewinnrate von nur 50 \% gegen einen zufällig spielenden Agenten erzielt, keinen strategischen Vorteil mehr besitzt ist.

Daraus ergibt sich folgende Formel zur Bewertung der Robustheit:

\[l = (r_n - r_s) / (r_n - 0.5)\]

Dabei bezeichnet $r_n$ die Gewinnrate des untersuchten Agenten im Spiel gegen einen zufällig spielenden Agenten im neutralen Szenario und $r_s$ die Gewinnrate des untersuchten Agenten im betrachteten Szenario mit Beeinträchtigung. Erzielt ein untersuchter Agent im neutralen und im betrachteten Szenario dieselbe Gewinnrate, wird $l$ 0. Erzielt ein untersuchter Agent im betrachteten Szenario eine Gewinnrate von 50 \%, so ist $l$ 1. Dazwischen verhält sich $l$ linear zu $r_s$. Somit ist ein untersuchtes Verfahren im betrachteten Szenario robuster, je kleiner der Wert von $l$ ist. Je größer $r_n$ ist, desto präziser lässt sich $l$ berechnen.

Bei den durchgeführten Messungen handelt es sich um Stichproben mit stochastisch bedingt variierenden Ergebnissen. Um dennoch eine Interpretierbarkeit der Messergebnisse zu gewährleisten, werden im Rahmen dieser Arbeit Konfidenzintervalle mit einem Konfidenzniveau von 95 \% eingesetzt. Sie enthalten mit einer Wahrscheinlichkeit von 95 \% den wahren Wert. Zur Ermittlung der Konfidenzintervalle für die Mittelwerte der Spieldauer wird das verbreitete Verfahren von Neyman angewandt (\cite{Frost.2023}, S. 11 ff., \cite{Janzyk.2020}, S. 66 ff.). Die Konfidenzintervalle für die Gewinnraten werden mit Hilfe des Wilson-Verfahrens berechnet, das für die Berechnung von Anteilen in binomial verteilten Daten empfohlen wird \cite{Wallis.2013} \cite{Lawrence.2001}.

Um Robustheit gegenüber Unsicherheit bezüglich Aktionen zu untersuchen, wird die Unsicherheit bezüglich Aktionen so abgebildet, dass, dass eine durch einen Agenten gewählte Aktion mit einer variierenden Wahrscheinlichkeit durch eine zufällige Aktion ersetzt wird. Beträgt diese Wahrscheinlichkeit 0 \%, verhält sich der Agent ungestört, bei 100 \% spielt der Agent wie ein zufällig spielender Agent. Interessant zu beobachten ist es also, wie sich der Verlauf des Gewinnratenverlustes zwischen 0 \% und 100 \% verhält. Der MCTS-Agent wird insofern eingeschränkt, dass er sich in seiner langfristigen Planung nicht darauf verlassen kann, dass er die Aktionen wählt, die er für am besten hält. Der RL-Agent hingegen in der Form, als dass er nicht immer die Aktionen durchführt, die sich im Training bewährt haben. Es gibt keine Hinweise darauf, dass ein Verfahren in diesem Szenario besser abschneidet als das andere. Durch die experimentelle Bewertung soll untersucht werden, ob dennoch Unterschiede auftreten.

Unsicherheit bezüglich Beobachtungen wird im Rahmen der Arbeit so auf Vier Gewinnt übertragen, dass die Agenten nicht den tatsächlichen Zustand des Spielfeldes beobachten können, sondern in den Beobachtungen im Vergleich zum tatsächlichen Zustand stets ein Spielstein oder mehrere Spielsteine hinzugefügt oder entfernt werden, sodass dadurch illegale Zustände entstehen, die im normalen Spiel nicht vorkommen können. Dabei ist zu beachten, dass sich bei bestimmten Modifikationen des Spielfeldzustands auch ändert, welche Aktionen möglich sind und welche nicht. Da es viele Probleme gibt, bei denen unabhängig vom Zustand immer alle Aktionen des Aktionsraums möglich sind, sollen die Messungen unabhängig von dieser Eigenschaft von Vier Gewinnt sein. Das bedeutet, dass bei den stattfindenden Modifikationen des Zustands nie der letzte Stein einer Spalte platziert, und auch kein Stein aus einer vollen Spalte entfernt werden darf. Die Anzahl der hinzuzufügenden oder zu entfernenden Steine wird bei den Messungen variiert.

Es sind dabei vor allem die Beobachtungen interessant, die keine legalen Zustände des Spiels darstellen. Denn bei gleich gut funktionierenden Agenten besteht die Erwartung, dass sie bei der Beobachtung eines falschen legalen Zustands, gleichwertig ungünstige Entscheidungen treffen. Werden fälschlicherweise illegale Zuständen beobachtet, lässt sich die Hypothese bilden, dass dies vor allem für den RL-Agenten problematisch sein könnte, da dessen neuronales Netzwerk lediglich auf Grundlage von legalen Zuständen trainiert wurde, und nicht erfolgreich auf illegale Zustände generalisieren kann, weil bei der Beobachtung von illegalen Zuständen Kombinationen von Neuronen aktiviert werden, dessen Gewichte im Training nicht entsprechend optimiert wurden, sodass die Ausgabe des neuronalen Netzwerks stark verfälscht wird. Der MCTS-Agent hingegen wird bei illegalen Zuständen weiterhin versuchen, den bestmöglichen Zug zu ermitteln. Seine Leistungsfähigkeit wird durch die fehlerhaften Eingabewerte gestört, seine grundsätzliche Funktionsweise ist jedoch weiterhin gegeben.

Da bei Vier Gewinnt, wie in Kapitel \ref{vier-gewinnt} erwähnt, der Spieler, der den ersten Stein platziert, einen Vorteil hat, wechselt bei den Messungen, sofern nicht anders gekennzeichnet, zu Beginn jedes Spiels das Recht, den ersten Zug zu durchzuführen.

Es ist zu beachten, dass die Agenten auf einem ähnlichen und gewissermaßen starken Level spielen. Denn bei Agenten, die ohnehin nicht stark spielen, wird es schwierig sein, in den verschiedenen Szenarien zur Untersuchung der Robustheit eine aussagekräftige Änderung in den Gewinnraten zu messen. Außerdem ist anzunehmen, dass ein stärkerer Agent Entscheidungen trifft, die längerfristig zum Erfolg führen, sodass er auch die Beeinträchtigungen in den verschiedenen Szenarien besser wegstecken kann als ein schwächerer Agent. Als Indikator für die Spielstärke der Agenten wird die durchschnittliche Gewinnrate und Spieldauer im Spiel gegen zufällig spielende Agenten bei abwechselndem Anzugsrecht und im Spiel gegen sich selbst bei konstantem Anzugsrecht verwendet. Im Spiel gegen zufällig spielende Agenten sollten starke Agenten hohe Gewinnraten bei kurzer Spieldauer erzielen. Im Spiel gegen sich selbst sollte der erste Spieler aufgrund des Vorteils des Anzugsrechts eine hohe Gewinnrate erzielen und auch die Spieldauer wird sich verlängern, da der Gegenspieler auch besser verteidigt, sodass komplexere Spielsituationen entstehen und sich die Entscheidung des Spiels hinauszögert. Zur Beurteilung der Spielstärke der Agenten erfolgt außerdem eine kurze qualitative Analyse im Spiel gegen einen Menschen.
