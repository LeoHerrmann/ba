In diesem Kapitel wird erklärt, wie eine Messumgebung aufgesetzt wurde, um die Robustheit der Lösungsverfahren empirisch zu bewerten. Diese Messumgebung ermöglicht es, zwei Agenten, die die zu untersuchenden Ansätze implementieren, das Spiel wiederholt gegeneinander spielen zu lassen. Die Spiele werden unter verschiedenen Szenarien durchgeführt, die jeweils auf ein in Kapitel \ref{robustheit} definierten Aspekt der Robustheit abzielen. Es werden die Gewinnraten gemessen, worüber Aussagen darüber getroffen werden können, welches der beiden Verfahren in den verschiedenen Szenarien stärker und damit robuster ist. Folgende Szenarien werden untersucht:

\begin{itemize}
	\item Unsicherheit bezüglich Aktionen: Die Aktionen der Agenten bestehen darin, dass sie den nächsten Spielstein in eine freie Spalte des Spielfelds hineinwerfen, die sie auf Grundlage des beobachteten Spielfeldzustands auswählen. Dieses Szenario führt Unsicherheit bezüglich Aktionen ein, indem für einen Agenten der Spielstein mit einer bestimmten Wahrscheinlichkeit nicht in die ausgewählte Spalte, sondern in eine zufällige freie Spalte fällt.
	
	\item Unsicherheit bezüglich Beobachtungen: Die Agenten wählen zwischen möglichen Aktionen basierend auf deren Beobachtungen des Spielfelds. In diesem Szenario erhalten die Agenten fehlerhafte Informationen über das Spielfeld. Jedes Feld besitzt dabei eine bestimmte Wahrscheinlichkeit mit der nicht dessen tatsächlicher Zustand (leer, besetzt durch Spieler 1, besetzt durch Spieler 2) erkannt wird, sondern ein zufälliger Zustand.
	
	Es ist anzumerken, dass hierbei aus dem MDP ein Partially Observable MDP (POMDP) wird, also ein MDP dessen Umgebung nur teilweise oder fehlerhaft beobachtbar ist. Der betroffene Agent kann nicht mit Sicherheit gesagt werden, in welchem Zustand er sich gerade befindet. Zusätzlich zum MDP enthält das POMDP ein Observation Modell O(s, o), das die Wahrscheinlichkeit beschreibt, eine Beobachtung o im Zustand s zu machen. POMDPs sind wesentlich komplizierter gezielt zu lösen, in der realen Welt jedoch wesentlich häufiger anzutreffen. Es gibt Optimierungen von MCTS und bestimmte RL-Methoden, über die POMDPs gezielt gelöst werden können, zum Beispiel indem für eine Entscheidung nicht nur der aktuelle Zustand, sondern die Historie der Zustände betrachtet wird (\cite{Russell.2020}, S. 588 ff.). Da es in dieser Arbeit darum geht, zu untersuchen, inwiefern grundsätzliche Eigenschaften von symbolischen Algorithmen und Reinforcement Learning Robustheit beeinflussen, werden diese gezielten Lösungen zur Vereinfachung nicht betrachtet. Beide Agenten gehen davon aus, dass es sich bei dem fehlerhaften Bild um den tatsächlichen Zustand des Spiels handelt, auch wenn der beobachtete Zustand gemäß der Spielregeln nicht erreicht werden könnte.
	
	\item Unsicherheit bezüglich Dynamik der Umgebung: Die Agenten erwarten ein bestimmtes Verhalten von der Umgebung, das durch die Spielregeln abgebildet wird. Der MCTS-Agent führt Simulationen anhand dieser Erwartungen aus und der RL-Agent wird auf Grundlage dieser Erwartungen trainiert. In diesem Szenario werden die Erwartungen an das Verhalten der Umgebung gebrochen, indem ein bestimmter Spieler mit einer bestimmten Wahrscheinlichkeit zwei Züge hintereinander durchführt.
	
	% \item Unsicherheit bezüglich Dynamik der Umgebung: Mit einer bestimmten Wahrscheinlichkeit führt der Gegenspieler nicht den Zug aus, den er für am besten hält, sondern einen zufälligen Zug. (Wurde rausgenommen, weil das quasi das gleiche ist wie Unsicherheit bezüglich Aktionen) nur von der anderen Perspektive.
\end{itemize}

In den verschiedenen Szenarien gelten die veränderten Bedingungen nur für jeweils einen Agenten. Wenn beide Agenten gleichzeitig von den veränderten Bedingungen betroffen wären, könnten nur Aussagen über die relative Robustheit zueinander getroffen werden. Dadurch dass die veränderten Bedingungen nur für jeweils ein Verfahren auf einmal gelten, kann genau gesagt werden, welches Verfahren wie stark betroffen ist.

Die Messungen werden nicht nur unter Szenarien mit veränderten Bedingungen durchgeführt, sondern auch in einer neutralen Umgebung ohne veränderte Bedingungen, um eine Grundlage für die Analyse der Ergebnisse zu bilden.

Um einen fairen Vergleich zu gewährleisten, müssen die Agenten in der neutralen Umgebung gleich stark sein. Dazu werden die Parameter der Agenten (Anzahl der Simulationen beim MCTS Agenten, Anzahl der durchlaufenen Self-Play-Trainingsepisonden beim RL-Agenten) so eingestellt, dass sie in der neutralen Umgebung im Spiel gegeneinander jeweils eine Gewinnrate von etwa 50\% aufweisen.

Außerdem ist es wichtig, dass die Agenten auf einem gewissen starken Level spielen. Denn bei Agenten, die ohnehin nicht stark spielen, wird es schwierig sein, in den verschiedenen Szenarien zur Untersuchung der Robustheit eine aussagekräftige Änderung in den Gewinnraten zu messen. Als Indikator für die Spielstärke der Agenten wird die durchschnittliche Spieldauer verwendet. Es wird angenommen, dass starke Agenten weniger Fehler machen und ihre Züge strategischer wählen, sodass komplexere Spielsituationen entstehen und sich die Entscheidung des Spiels hinauszögert.

Da bei Vier Gewinnt, wie in Kapitel \ref{vier-gewinnt} erwähnt, der Spieler, der den ersten Stein platziert, einen Vorteil hat, wechselt zu Beginn jedes Spiels das Recht, den ersten Zug zu machen.
