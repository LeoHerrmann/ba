In dieser Arbeit wird am Beispiel des Brettspiels Vier Gewinnt untersucht, inwiefern symbolische Algorithmen oder Reinforcement-Learning-Verfahren robuster sind. Dazu wurden zwei Agenten implementiert, die das Spiel selbstständig spielen. Beim ersten Agenten kommt der symbolische Alorithmus Monte Carlo Tree Search (MCTS) zum Einsatz. Dem zweiten Agenten liegt das RL-Verfahren Proximal Policy Optimization (PPO) zugrunde. Die Robustheit der Agenten wird quantifiziert, indem der Verlust der Gewinnrate gegen einen zufällig spielenden Agenten gemessen wird, während die zu untersuchenden Agenten zwei verschiedenen Szenarien mit ungünstigen Bedingungen ausgesetzt sind, die auf verschiedene Aspekte von Robustheit abzielen. Im ersten Szenario liegt Unsicherheit bezüglich Aktioinen vor, was bedeutet, dass die Agenten keine vollständige Kontrolle darüber haben, in welche Spalte sie ihre Spielsteine platzieren. Im zweiten Szenario erhalten die Agenten fehlerhafte Informationen über das Spielfeld, somit liegt Unsicherheit bezüglich Beobachtungen vor.

Es wird gezeigt, dass unter den beiden implementierten Agenten der MCTS-Agent im Szenario Unsicherheiten bezüglich Aktionen robuster ist als der PPO-Agent. Für das Szenario Unsicherheiten bezüglich Beobachtungen wurde dies bis zu einem gewissen Ausmaß an Unsicherheiten ebenfalls beobachtet. Die Ergebnisse dieser Arbeit lassen sich jedoch nicht auf den Vergleich von Robustheit zwischen MCTS und PPO unabhängig von der konkreten Implementierung oder dem Anwendungsfall übertragen, da der implementierte PPO-Agent für einen fairen Vergleich unzureichend trainiert wurde. Dafür und um allgemeine Aussagen über den Verlgleich der Robustheit zwischen symbolischen Algorithmen und Reinforcement Leraning zu übertragen, sind weitere Untersuchungen erforderlich.

\newpage