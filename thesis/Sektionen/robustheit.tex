\label{robustheit}

Der Begriff Robustheit wird durch das IEEE Standard Glossary of Software Engineering Terminology definiert als \glqq Der Grad, zu dem ein System oder eine Komponente in der Lage ist, unter fehlerhaften Eingaben oder belastenden Umgebungsbedingungen korrekt zu funktionieren\grqq{} (\cite{IEEE.1990}, S. 64).

In der Studie von Moos et al. wird eine Übersicht zu verschiedenen Herangehensweisen zur Verbesserung der Robustheit von RL-Verfahren geschaffen. Der Begriff Robustheit wird insofern konkret auf RL übertragen, als dass es darum geht, wie zuverlässig RL-Verfahren funktionieren, wenn das Verhalten der Umgebung teilweise unbekannt ist. Das ist insbesondere relevant, weil RL-Modelle zunehmend in der realen physischen Welt angewendet werden, während sie weiterhin aus Kosten- oder Zeitgründen in simulierten Umgebungen trainiert werden. Bei der Anwendung dieser Modelle in der realen Welt kommt es dazu, dass deren Leistungsfähigkeit sinkt, da reale Einsatzszenarien Eigenschaften besitzen, die in der Simulation nicht vollständig abgebildet werden. So zum Beispiel in der Navigation basierend auf Kamerabildern, bei der zeitweise das Sichtfeld blockiert sein kann, oder bei der Kollaboration von Robotern und Menschen, bei der die Roboter in der Lage sein müssen, auf die vielschichtigen Absichten des Menschen reagieren zu können \cite{Moos.2022}\cite{Ni.2021}.

Das Ziel beim robusten Reinforcement Learning besteht darin, ein Regelwerk zu finden, das auch unter ungünstigen Bedingungen möglichst gute Entscheidungen trifft. In der Studie wird jede untersuchte Herangehensweise einer von vier Kategorien zugeordnet, je nachdem in welcher Hinsicht dadurch erhöhte Robustheit erzielt werden soll. Die Kategorien betreffen jeweils unterschiedliche Aspekte des modellierten MDPs.

Die erste Kategorie lautet Robustheit gegenüber Unsicherheit bezüglich Aktionen. Die hier zugeordneten Methoden zur Erhöhung der Robustheit zielen darauf ab, das Verhalten eines Agenten zu verbessern, wenn er in Szenarien eingesetzt wird, in denen er sich nicht darauf verlassen kann, dass die Aktionen, für die er sich entscheidet, auch tatsächlich durchgeführt werden. So ist es beispielsweise der Fall, wenn ein RL-Agent einen Roboterarm steuert, auf den plötzliche Stöße wirken (\cite{Moos.2022}, S. 301 ff.). Ein solches Szenario kann in Vier Gewinnt abgebildet werden, indem beispielsweise mit einer bestimmten Wahrscheinlichkeit nicht die Aktion durchgeführt wird, für die sich die Agenten entscheiden, sondern eine zufällige Aktion.

Bei der Kategorie Robustheit gegenüber Unsicherheit bezüglich Beobachtungen besteht das Ziel darin, das Verhalten eines Agenten zu optimieren, wenn sich der Agent nicht darauf verlassen kann, dass der beobachtete Zustand auch der tatsächliche Zustand ist. Das ist zum Beispiel der Fall, wenn Entscheidungen basierend auf Sensordaten getroffen werden, die fehlerhaft oder verrauscht sind (\cite{Moos.2022}, S. 304 ff.). Auch das lässt sich in Vier Gewinnt abbilden, indem die Agenten nicht den tatsächlichen Zustand, sondern eine manipulierte Version als Entscheidungsgrundlage erhalten.

Es ist anzumerken, dass hierbei aus dem MDP ein Partially Observable MDP (POMDP) wird, also ein MDP, dessen Umgebung nur teilweise oder fehlerhaft beobachtbar ist. Der betroffene Agent kann nicht mit Sicherheit sagen, in welchem Zustand er sich gerade befindet. Zusätzlich zum MDP enthält das POMDP ein Observation Modell $O(s, o)$, das die Wahrscheinlichkeit beschreibt, eine Beobachtung $o$ im Zustand $s$ zu erhalten. POMDPs sind komplizierter gezielt zu lösen, in der realen Welt jedoch wesentlich häufiger anzutreffen. Es gibt Optimierungen von MCTS und bestimmte RL-Methoden, über die POMDPs gezielt gelöst werden können, zum Beispiel indem für eine Entscheidung nicht nur der aktuelle Zustand, sondern die Historie der Zustände betrachtet wird (\cite{Russell.2020}, S. 588 ff.). Da es in dieser Arbeit darum geht, zu untersuchen, inwiefern grundsätzliche Eigenschaften von symbolischen Algorithmen und Reinforcement Learning Robustheit beeinflussen, werden diese gezielten Lösungen zur Vereinfachung nicht betrachtet.

Bei den Herangehensweisen der Kategorien Robustheit gegenüber Störungen und Robustheit gegenüber Unsicherheiten bezüglich der Übergangsfunktion soll das Verhalten von Agenten unter leicht verschiedenen physikalischen Parametern wie Masse, Reibung, oder Gravitation verbessert und die Toleranz gegenüber Modellierungsfehlern im Training erhöht werden (\cite{Moos.2022}, S. 291 ff.). Es ist nicht möglich, diese Arten von Robustheit bei Vier Gewinnt zu untersuchen, da es sich bei dabei um ein theoretisches und kein physikalisches Problem handelt, und keine vergleichbaren Parameter variiert werden können.

Es ist anzumerken, dass die Kategorien nicht streng voneinander getrennt sind. Es liegt nahe, ein Szenario, bei dem ein Agent anstelle der durch ihn bestimmten Aktion eine andere Aktion durchführt, der Kategorie Robustheit gegenüber Unsicherheit bezüglich Aktionen zuzuordnen. Es kann aber auch der Kategorie Unsicherheit bezüglich der Übergangsfunktion zugeordnet werden, mit der Begründung, dass das Szenario auch so betrachtet werden kann, dass der Agent die gewählte Aktion durchführt, aber dadurch in einen anderen Zustand gerät als erwartet (\cite{Moos.2022}, S. 301).

Ein Konzept, das beim robusten Reinforcement Learning häufig zum Einsatz kommt, besteht darin, das zu lösende Single-Agent-Problem als Multi-Agent-Problem zu betrachten, bei dem während des Trainings des RL-Agenten, dessen Ziel die Lösung des ursprünglichen Problems ist, auch ein Kontrahent trainiert wird, der durch bestimmte Störfaktoren, z.B. die Änderung von physikalischen Parametern der Umgebung, die Leistungsfähigkeit des ersten Agenten senken soll (\cite{Moos.2022}, S. 290). Dadurch arbeiten auf Robustheit optimierte Verfahren häufig pessimistischer als nicht auf Robustheit optimierte Verfahren, sodass es je nach konkretem Verfahren vorkommen kann, dass das Verfahren unter optimalen Bedingungen schlechtere Ergebnisse erzielt als das nicht auf Robustheit optimierte Verfahren. Wenn die Trainingsumgebung die Anwendungsumgebung gut abbildet, können nicht-robuste Verfahren bessere Ergebnisse erzielen, als solche, die auf Robustheit optimiert sind (\cite{Moos.2022}, S. 293).

Da sich die verschiedenen Arten der Unsicherheit auf Aspekte des MDPs beziehen, ergibt sich die Möglichkeit, in dieser Hinsicht nicht nur RL-Verfahren, sondern auch MDP-lösende symbolische Algorithmen auf Robustheit zu untersuchen.
