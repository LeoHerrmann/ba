\subsection{Messumgebung}

\label{messumgebung}

% PettingZoo-Bibliothek

Als Grundlage für die Messumgebung dient die Open Source Python-Bibliothek PettingZoo, die zum Entwickeln und Testen von MARL-Systemen konzipiert wurde. Sie stellt eine einheitliche Schnittstelle zu Umgebungen bereit, in der Agenten miteinander interagieren können.

Die Umgebungen definieren den Rahmen, in dem die Agenten miteinander interagieren. Sie weisen ein bestimmtes Verhalten auf und definieren unter anderem, unter welchen Bedingungen welche Aktionen möglich sind, Belohnungen verteilt werden und in welcher Form die Agenten Informationen über den Zustand der Umgebung erhalten. Es existieren eine Reihe von vorgefertigten Umgebungen, darunter welche, die kooperative Probleme zum Benchmarking von MARL-Systemen oder auch rundenbasierte Spiele wie Vier Gewinnt abbilden.

Die durch PettingZoo bereitgestellte Schnittstelle ermöglicht es, aus Sicht eines Agenten den aktuellen Zustand der Umgebung zu beobachten, die im aktuellen Zustand möglichen Aktionen und erhaltenen Belohnungen zu ermitteln und eine Aktion auszuwählen, die in der Umgebung durchgeführt werden soll.

Für alle Agenten der Umgebung kann benutzerdefinierte Logik eingebunden werden, die bestimmt, wie sie ihre Aktionen wählen. Vorgesehen sind dabei RL-Modelle, es können jedoch auch symbolische Algorithmen eingesetzt werden, darunter auch welche, die ihre Entscheidungen rein zufällig oder unter Einbezug von menschlichen Eingaben treffen \cite{Farama.2025}.

% Implementierung der Messumgebung

Die Implementierung der Messumgebung baut auf der offiziellen Implementierung der Vier-Gewinnt-Umgebung von PettingZoo auf. Die Messumgebung tut dabei nichts anderes, als wiederholt zwei Agenten mit bestimmten Lösungsansätzen das Spiel spielen zu lassen und dabei die Gewinnraten und Spieldauer zu messen. Die Open-Source-Eigenschaft ermöglicht es, den Quellcode zu modifizieren. Davon wird im weiteren Verlauf der Arbeit Gebrauch gemacht, unter anderem, um die verschiedenen Szenarien zur Untersuchung von Robustheit abzubilden.

Im Zuge der Realisierung der Messumgebung ist aufgefallen, dass wenn zwei Agenten (Spieler 0 und Spieler 1) alle Aktionen im Spiel mit derselben Wahrscheinlichkeit rein zufällig wählen, nach 1000 Spielen Spieler 0 mit 55,20 \% gegenüber Spieler 1 mit 44,30 \% eine wesentlich höhere Gewinnrate erzielt.

Um sicherzustellen, dass es sich dabei nicht um einen stochastisch bedingten Messfehler handelt, wurde Binomialtests durchgeführt. Bei gleichen Chancen für beide Spieler wird eine Gewinnrate von jeweils 50 \% erwartet. Die Tests mit $n = 995$ entschiedenen Spielen, wovon $k_1 = 552$ durch den ersten Spieler und $k_2 = 443$ durch den zweiten Spieler gewonnen wurden, liefern unter der Hypothese, dass die beiden Spieler gleiche Chancen haben, p-Werte von 0,0060 und 0,0006, was unter dem Signifikanzniveau von $\alpha = 0.05$ liegt. Dadurch wird die Hypothese verworfen, was bedeutet, dass die Differenz der Gewinnraten trotz stochastischer Schwankungen signifikant ist.

\begin{figure}[ht!]%[!tbp]
	\begin{subfigure}[b]{0.48\textwidth}
		\includegraphics[width=\textwidth]{Bilder/random_vs_random_constant_player_order_graph_win_rates.png}
		\caption{Gewinnrate.}
		\label{fig:f1}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.48\textwidth}
		\includegraphics[width=\textwidth]{Bilder/random_vs_random_constant_player_order_graph_game_length.png}
		\caption{Durchschnittliche Spieldauer.}
		\label{fig:f2}
	\end{subfigure}
	\caption{Gewinnrate und durchschnittliche Spieldauer bei konstanter Spielerreihenfolge.}
\end{figure}

Das lässt sich dadurch erklären, dass die Vier-Gewinnt-Umgebung so implementiert ist, dass Spieler 0 immer der Spieler ist, der den ersten Stein setzen darf. Er ist damit seinem Gegenspieler immer einen Spielzug voraus, was die Wahrscheinlichkeit erhöht, als erstes Vier Steine in eine Reihe zu bekommen. An dieser Stelle sei nochmals zu erwähnen, dass bei Vier Gewinnt der erste Spieler bei optimaler Spielweise stets gewinnen kann. Um ausgeglichene Messungen zu gewährleisten, muss daher sichergestellt werden, dass sich im Rahmen der Messungen die beiden Spieler mit dem ersten Zug abwechseln.

Die Vier-Gewinnt-Umgebung von PettingZoo wurde daher erweitert, um einen Parameter entgegenzunehmen und zu verarbeiten, der bestimmt, welcher Spieler anfangen soll. Die Messumgebung wechselt den Wert des Parameters nach jedem Spiel durch. Nach dieser Änderung weisen die Spiele wesentlich ausgeglichenere Ergebnisse auf. Spieler 0 gewinnt 50,30 \% und Spieler 1 49,50 \% der Spiele. Der zuvor bereits durchgeführte Binomialtest liefert mit den Werten $n = 998$, $k_1 = 503$ und $k_2 = 495$ p-Werte von jeweils 0.8246, was deutlich über dem Signifikanzniveau von $\alpha = 0.05$ liegt, wodurch die Hypothese behalten wird. Die durchschnittliche Spieldauer bleibt dabei mit 22,48 Zügen vor der Änderung gegenüber 22,26 Zügen nach der Änderung nahezu unverändert.

\begin{figure}[ht!]%[!tbp]
	\begin{subfigure}[b]{0.48\textwidth}
		\includegraphics[width=\textwidth]{Bilder/random_vs_random_alternating_player_order_graph_win_rates.png}
		\caption{Gewinnrate.}
		\label{fig:f3}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.48\textwidth}
		\includegraphics[width=\textwidth]{Bilder/random_vs_random_alternating_player_order_graph_game_length.png}
		\caption{Durchschnittliche Spieldauer.}
		\label{fig:f4}
	\end{subfigure}
	\caption{Gewinnrate und durchschnittliche Spieldauer bei abwechselnder Spielerreihenfolge.}
\end{figure}



\subsection{MCTS-Agent}

Bei der Implementierung des MCTS-Agenten, diente \glqq Deep Learning and the Game of Go\grqq{} (\cite{Ferguson.January2019}, Kapitel 4.5) als Orientierung. Wie im genannten Werk besteht die im Rahmen dieser Arbeit entstandenen Implementierung aus zwei Klassen. Einer Klasse \texttt{MctsNode}, die einen Knoten im MCTS-Baum abbildet, und einer weiteren Klasse \texttt{MctsAgent}, die den Agenten repräsentiert und die wesentliche Logik des Algorithmus beinhaltet.

Die Klasse \texttt{MctsNode} besitzt dabei unter anderem folgende Attribute:
\begin{itemize}
\item \texttt{parent: MctsNode} und \texttt{children: list[MctsNode]}: Sie verwalten Beziehungen zu anderen Instanzen der Klasse \texttt{MctsNode}, sodass sie zusammen den MCTS-Baum abbilden.
\item \texttt{visitation\char`_count: int } und \texttt{total\char`_reward: int}: Diese Werte werden zur Berechnung der UCT-Werte in der Selection-Phase benötigt und in der Backpropagation-Phase aktualisiert.
\item \texttt{state: numpy.ndarray(6, 7, 2)}: Dabei handelt es sich um den Spielfeldzustand, den der Knoten repräsentiert. Das Format ist dabei dasselbe wie das, mit dem PettingZoo Beobachtungen über das Spielfeld zur Verfügung stellt. Dieses Attribut dient als Ausgangspunkt für die zufälligen Simulationen.
\end{itemize}

Die zentrale Methode der Klasse \texttt{MctsAgent} ist die Methode \texttt{determine\char`_action(self, state: numpy.ndarray(6, 7, 2)) -> int}. Sie nimmt den Zustand des Spielfelds entgegen, führt den MCTS-Algorithmus durch und gibt eine Zahl zurück, die die Aktion widerspiegelt, die auf Grundlage des Algorithmus gewählt werden soll. Zunächst wird darin ein Objekt der Klasse \texttt{MctsNode} initialisiert, dessen \texttt{state}-Attribut der beobachtete Zustand \texttt{state} zugewiesen wird. Dieses Objekt stellt den Wurzelknoten des MCTS-Baums dar. Anschließend werden n mal folgende Methoden wiederholt, wobei n die Anzahl der pro Entscheidung durchzuführenden Simulationen ist, die über den Konstruktor der Klasse konfiguriert werden kann:

\begin{itemize}
\item \texttt{select(root\char`_node: MctsNode) -> MctsNode}: Vom zuvor definierten Wurzelknoten werden per UCT-Formel solange Kinder ausgewählt, bis ein Knoten erreicht wurde, der ein Endzustand ist oder nicht vollständig expandiert ist, also weniger Kindknoten als Aktionen hat, die von dem dem Knoten entsprechenden Zustand möglich sind. Die UCT-Konstante kann dabei über den Konstruktor konfiguriert werden. Wenn es sich bei dem erreichten Knoten um einen Endzustand handelt, wird dieser Knoten zurückgegeben. Ansonsten, wird von diesem Knoten ein zufälliger legaler Spielzug ausgeführt und ein neuer Knoten, der den dadurch erreichten Zustand abbildet, wird als Kindknoten hinzugefügt. Dies entspricht dem Expansion-Schritt des MCTS-Algorithmus. Zurückgegeben wird dann der neu hinzugefügte Knoten.
\item \texttt{simulate(selected\char`_node: MctsNode) -> int}: Diese Methode nimmt den in der \texttt{select}-Methode ausgewählten Knoten entgegen, das Spiel wird ab dem Zustand, den der Knoten repräsentiert, zu Ende gespielt, und das Ergebnis des Spiels wird zurückgegeben.
\item \texttt{backpropagate(selected\char`_node: MctsNode, result: int) -> None}: Von ausgewählten Knoten wird das Attribut \texttt{visitation\char`_count} erhöht und das Ergebnis der Simulation zu \texttt{total\char`_rewards} addiert. Dieser Vorgang wird jeweils für alle Elternknoten durchgeführt, bis der Wurzelknoten erreicht wurde.
\end{itemize}

Nach n Wiederholungen wird eine Zahl zurückgegeben, die die Aktion repräsentiert, die vom Wurzelknoten zum direkten Kindknoten führt, dessen \texttt{visitation\char`_count}-Attribut den höchsten Wert hat.

% Anpassungen an der PettingZoo Umgebung

Für den Algorithmus wird ein Abbild für die Dynamik des Spiels benötigt, das unter anderem die Spielregeln, Gewinnbedingungen oder mögliche Aktionen in Abhängigkeit des aktuellen Zustands enthält. Dafür kommt in dieser Implementierung die Vier-Gewinnt-Umgebung von PettingZoo zum Einsatz. Diese enthält ein genau solches Abbild, und dadurch wird Aufwand in der Implementierung gespart. Es hat sich herausgestellt, dass an der Umgebung zwei Modifikationen notwendig sind, da die PettingZoo-Umgebungen in erster Linie zum Training von RL-Agenten konzipiert sind, und nicht um als Modell für symbolische Algorithmen oder modellbasierte RL-Verfahren zu dienen.

Zunächst wird keine Funktionalität unterstützt, um eine PettingZoo-Umgebung mit einem bestimmten Zustand zu initialisieren, was jedoch in der Klasse \texttt{MctsAgent} beispielsweise vor der Durchführung von Simulationen notwendig ist. Die Methode \texttt{reset(self, seed: int, options: dict) -> None} der Vier-Gewinnt-Umgebung wurde daher erweitert, um im \texttt{options}-Parameter nach dem Schlüssel \texttt{``state''} zu suchen, unter dem der gewünschte Zustand abgelegt werden kann, und ggf. entsprechend verarbeitet wird. Der Zustand wird dabei in derselben Form erwartet, wie PettingZoo seine Beobachtungen liefert.

Eine weitere Herausforderung bestand darin, dass PettingZoo-Umgebungen Beobachtungen stets perspektivisch aus Sicht des Agenten liefern, der aktuell am Zug ist. Im Fall von Vier Gewinnt bestehen die Beobachtungen aus einem Array, das das Spielfeld repräsentiert. Dieses Array enthält sechs weitere Arrays, die jeweils eine Reihe des Spielfelds abbilden, wobei jedes dieser Arrays sieben Felder enthält, die einem Feld in der jeweiligen Reihe entsprechen. Jedes dieser Felder ist ein Array bestehend aus zwei Elementen, die jeweils die Werte 0 und 1 annehmen können. Wenn das erste Element 1 ist, bedeutet das, dass der Spieler, der aktuell am Zug ist, einen Stein an der entsprechenden Position platziert hat. Wenn das zweite Feld 1 ist, bedeutet das, dass der Gegenspieler einen Stein platziert hat. 0 bedeutet, dass der entsprechende Spieler an der Position keinen Stein platziert hat \cite{Farama.2025}. Diese perspektivischen Beobachtungen erleichtern die Implementierung von RL-Agenten. Da die Vier-Gewinnt-Umgebung von PettingZoo jedoch nicht mit perspektivischen Beobachtungen, sondern mit einem globalen Zustand arbeitet, mussten Mechanismen implementiert werden, um diesen aus den perspektivischen Beobachtungen zu erzeugen. Dazu speichert die Klasse MctsNode, welcher Spieler als Nächstes am Zug ist, und die reset-Methode der Umgebung berücksichtigt einen entsprechenden Schlüssel im options-Parameter.

Aufgrund der zeitlichen Beschränkung dieser Arbeit wird auf Experimente bezüglich Optimierungen verzichtet. Daher wird als Auswahlstrategie in der Selection-Phase nach den Empfehlungen UCT mit $c=\sqrt{2}$ eingesetzt. Was die Expansion-Phase betrifft, wird sich ebenfalls an den Standard gehalten und nur einen und nicht mehrere Knoten hinzugefügt. In der Simulation-Phase werden Light-Playouts und keine Heavy-Playouts eingesetzt, da Light-Playouts ohne Wissen über das konkret zu lösende Problem auskommen, sodass die Ergebnisse dieser Arbeit so weit möglich auf verschiedene Probleme angewandt werden können.

\subsubsection{Zeitlicher Aufwand von Entscheidungen des MCTS-Agenten}

Die in dieser Arbeit durchgeführten Messungen fanden auf einem Computer mit einer Intel Core i7 8650U CPU statt. Ein MCTS-Agent, der pro Entscheidung 5.000 Simulationen durchführt, benötigt dabei für jeden Zug etwa eine zehn Sekunden. In einem Spiel mit einer Länge von 20 Zügen rechnet der MCTS-Agent also 200 Sekunden. Für 200 Spiele, die im Rahmen dieser zeitlich begrenzten Arbeit an vielen Stellen als ausreichend für aussagekräftige Messungen betrachtet werden, werden für jeden beteiligten MCTS-Agenten mehr als elf Stunden benötigt. Die Rechenzeit verhält sich proportional zur Anzahl der durchgeführten Simulationen.

Es liegt nahe, den MCTS-Agenten durch Parallelisierung zu beschleunigen. Dadurch lässt sich die Rechenzeit proportional (ggf. sogar überproportional) zur auf der Maschine verfügbaren CPU-Ressourcen verkürzen, ohne dass die Ergebnisse dadurch beeinträchtigt werden (vgl. \cite{Chaslot.2008}). Dies ist vor allem bei Echtzeitanwendungen sinnvoll. Ein solcher Anspruch wird in dieser Arbeit jedoch nicht gestellt. Um die Komplexität des Algorithmus möglichst niedrig zu halten, wird auf die Parallelisierung von MCTS verzichtet. Stattdessen werden die Messungen parallelisiert und die Ergebnisse zusammengeführt.

\subsubsection{Quantitative Untersuchung}

Um einen Eindruck davon zu bekommen, wie sich die Spielstärke des implementierten MCTS-Agenten in Abhängigkeit von der Anzahl der für jede Entscheidung durchgeführten Simulationen zu bekommen, wurde eine qualitative Analyse des MCTS-Agenten durchgeführt, in der 200 Spiele gegen einen zufällig spielenden Agenten mit abwechselndem Anzugsrecht durchgeführt wurden und 200 Spiele gegen sich selbst mit konstantem Anzugsrecht. Diese Messungen wurden mit 50, 100, 250, 500, 750, 1000, 2500 und 5000 Simulationen pro Entscheidung durchgeführt.

\begin{figure}[ht!]%[!tbp]
	\begin{subfigure}[b]{0.48\textwidth}
		\includegraphics[width=\textwidth]{Bilder/mcts_vs_random_win_rate_vs_n_simulations.png}
		\caption{Gewinnrate.}
		\label{fig:f5}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.48\textwidth}
		\includegraphics[width=\textwidth]{Bilder/mcts_vs_random_game_length_vs_n_simulations.png}
		\caption{Durchschnittliche Spieldauer.}
		\label{fig:f6}
	\end{subfigure}
	\caption{Gewinnrate und durchschnittliche Spieldauer in Abhängigkeit von der Anzahl der Simulationen pro Entscheidung beim Spiel eines MCTS-Agenten gegen einen zufällig spielenden Agenten.}
\end{figure}

Die Gewinnrate, die der MCTS-Agent gegen den zufällig spielenden Agenten erzielt, beträgt bei 50 Simulationen pro Entscheidung bereits 98,5 \% und steigt bis 1000 Simulationen auf 100 \%, welche der Agent auch bei 2500 und 5000 Simulationen hält. Die mittlere Spieldauer startet bei 50 Simulationen mit 14.21 Zügen und flacht bei 2500 Simulationen mit 9,69 ab. Dass die Spieldauer bei 5000 Simulationen auf 9,72 steigt, dabei handelt es sich vermutlich um eine stochastisch bedingte Messungenauigkeit. Die kürzeste mögliche durchschnittliche Spieldauer beträgt dabei 7.5, denn damit der anziehende Spieler gewinnt kann, müssen mindestens sieben Steine platziert sein, bzw. acht Steine, damit der nachziehende Spieler gewinnen kann. Sie ist auch bei perfekter Spielweise des MCTS-Agenten schwer zu erreichen, da ein Spiel länger dauert, sobald sein Gegenspieler eine durch den MCTS-Agenten gebildete Kette blockiert. Aus den Messungen geht hervor, dass der MCTS-Agent einem zufällig spielenden Agenten bereits bei 50 Simulationen pro Entscheidung weit überlegen ist. Auch wenn die Gewinnrate ab 1000 Simulationen 100 \% beträgt, kann über die kürzer werdende Spieldauer eine Steigerung der Leistung beobachtet werden. Der MCTS-Agent entscheidet mit steigender Anzahl von Simulationen pro Entscheidung das Spiel schneller für sich.

\begin{figure}[ht!]%[!tbp]
	\begin{subfigure}[b]{0.48\textwidth}
		\includegraphics[width=\textwidth]{Bilder/mcts_vs_mcts_win_rate_vs_n_simulations.png}
		\caption{Gewinnrate.}
		\label{fig:f7}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.48\textwidth}
		\includegraphics[width=\textwidth]{Bilder/mcts_vs_mcts_game_length_vs_n_simulations.png}
		\caption{Durchschnittliche Spieldauer.}
		\label{fig:f8}
	\end{subfigure}
	\caption{Gewinnrate und durchschnittliche Spieldauer in Abhängigkeit von der Anzahl der Simulationen pro Entscheidung beim Spiel von zwei MCTS-Agenten gegeneinander.}
\end{figure}

Im Spiel gegen sich selbst erzielt der MCTS-Agent mit Anzugsrecht bei 50 Simulationen eine Gewinnrate von 60,0 \%. Sie steigt von dort auf 79,0 \% bei 5000 Simulationen pro Entscheidung. Bei 750 Simulationen sinkt die Gewinnrate auf 67,0 \% von 68,0 \% bei 500 Simulationen. Diese Beobachtung ist vermutlich stochastisch bedingt. Die durchschnittliche Spieldauer steigt zunächst rapide von 16,08 bei 50 Simulationen auf 18,99 Spieldauer bei 250 Simulationen und sinkt von dort auf 12,07 bei 5000 Simulationen. Aus der steigenden Gewinnrate und sinkenden Spieldauer geht hervor, dass der MCTS-Agent den Vorteil, das Anzugsrecht zu haben, unso konsequenter ausnutzen kann, je mehr Simulationen pro Entscheidung durchgeführt werden. Im Vergleich zu den Messungen aus Kapitel \ref{messumgebung}, in denen zwei zufällig entscheidende Agenten mit konstantem Anzugsrecht spielen, wird deutlich, dass dies bereits bei 50 Simulationen der Fall ist. Da der erste Spieler das Spiel theoretisch immer gewinnen kann, sollte die Gewinnrate mit steigender Simulationsanzahl zu 100 \% konvergieren.

\subsubsection{Qualitative Untersuchung}

Neben der quantitativen Analyse gegen zufällig spielende Agenten und gegen sich selbst wurde auch eine kurze qualitative Analyse im Spiel gegen einen menschlichen Spieler durchgeführt, um einen Eindruck über das strategische Spielverhalten des MCTS-Agenten zu gewinnen, das aus den quantitativen Analysen nicht hervorgeht. Der menschliche Spieler ist bei den Untersuchungen stets der Spieler, der den ersten Stein setzen darf. Er kann damit theoretisch jedes Spiel gewinnen und hat so mehr Kontrolle über den Spielverlauf, was die Analyse vereinfacht.

Ist der MCTS-Agent konfiguriert, um 500 Simulationen pro Entscheidung durchzuführen, lässt sich bereits beobachten, dass, wenn er bereits drei Steine in einer Kette platziert hat, stets den vierten Stein platziert, um zu gewinnen, sofern dies möglich ist. Wenn hingegen sein Gegenspieler drei Steine in einer Reihe hat, versäumt der Agent häufig, den vierten Stein zu blockieren, sodass der Gegenspieler das Spiel für sich entscheiden kann. Wird der MCTS-Agent nicht unter Druck gesetzt, wirken seine Züge häufig ziellos. Wenn er Angriffe vorbereitet, sind seine Züge leicht durchschaubar, was es als Mensch recht einfach macht, sie zu verteidigen. Es ist auch ohne strategische Kenntnisse des Spiels leicht möglich, gegen den MCTS-Agenten zu gewinnen. Ein Vorgehen das dabei häufig funktioniert, besteht darin, darauf abzuzielen, einen Stein in der vierten Reihe der mittleren Spalte platziert zu bekommen, und von dort aus eine diagonale Kette nach links unten oder rechts unten zu bilden. Manchmal geht dieser Plan nicht auf, jedoch entwickeln sich dadurch im Laufe des Spiels schnell andere offensichtliche Gewinnchancen.

Bei 2.500 Simulationen pro Entscheidung platziert der MCTS-Agent seinen ersten Stein meistens in der mittleren Spalte, sofern er frei ist, was laut Allis auch der stärkste Anfangszug ist \cite{Allis.1988}. Hat der Gegenspieler drei Steine in einer Kette positioniert, so wird diese Angriffsposition meistens durch den MCTS-Agenten verteidigt. Es macht sich auch bemerkbar, dass der Agent wesentlich aggressiver spielt. Platziert der Gegenspieler beispielsweise seine Steine zu Beginn des Spiels so, dass er keine Gefahr darstellt und die Bildung von Viererketten erst zum letztmöglichen Zeitpunkt verteidigt, baut der Agent Druck auf, sodass der Gegenspieler im restlichen Verlauf des Spiels häufig dazu gezwungen ist, einen bestimmten Zug zu wählen, um nicht im nächsten Zug zu verlieren. Verteilt der Gegenspieler seine ersten drei Steine auf die beiden gegenüberliegenden Ränder, erzeugt der Agent häufig eine Zwickmühle, in dem er seine ersten drei Steine jeweils in die unterste Reihe der drei mittleren Spalten platziert, sodass er im nächsten Zug gewinnen kann, indem er seinen Stein in der zweiten oder vorletzten Spalte platziert. Dieses Verhalten konnte bei 500 Simulationen nicht beobachtet werden. Auch hier gelingt die oben genannte Strategie, sich als Ziel zu setzen, eine Diagonale von einer unteren Ecke zur Position in der mittleren Spalte in der vierten Reihe zu bilden. Allerdings scheint der MCTS-Agent besser voraus zu planen, denn die Spiele dauern länger und sein Gegenspieler muss häufiger verteidigen.

Bei 5.000 Simulationen pro Zug setzt der MCTS-Agenten seinen ersten Stein, wenn möglich, stets in die mittlere Spalte. Es ist eine verstärkte Bildung von Zwickmühlen zu bemerken, sofern sie durch den Gegenspieler nicht frühzeitig erkannt und verhindert werden. Durch seine noch aggressivere Spielweise, ist es nur unter besonderer Anstrengung möglich, als menschlicher Spieler ohne Kenntnisse über die optimale Spielweise gegen den MCTS-Agenten zu gewinnen.

Meistens geschieht dies durch die Ausnutzung einer Schwachstelle, die der MCTS-Agent unabhängig von der Anzahl der Simulationen pro Entscheidung (auch bei 10.000 Simulationen) aufzuweisen scheint: Hat der Gegenspieler drei Steine in einer Kette platziert und es droht, dass er im nächsten Zug den vierten Stein platziert und damit das Spiel gewinnt, so blockiert der MCTS-Agent die Platzierung nicht in jedem Fall. Besonders deutlich wird dies, wenn der Gegenspieler drei Steine in einer Spalte aufeinander liegen hat, während der MCTS-Agent zwei Steine in einer anderen Spalte aufeinander liegen hat, und das Spielfeld ansonsten leer ist. Anstatt den Gegenspieler zu blockieren, setzt er häufig den dritten Stein in seine Spalte.

Um diese Schwachstelle weiter zu untersuchen, wurde der MCTS-Agent konfiguriert, um 10.000 Simulationen pro Entscheidung durchzuführen und es wurde mit verschiedenen Werten für $c_{UCT}$ gemessen, wie oft sich der MCTS-Agent in dieser Situation dafür entscheidet, den unmittelbaren Gewinn des Gegenspielers zu verhindern und wie oft er es bevorzugt, stattdessen den dritten Stein in seine Spalte zu platzieren und damit den Gewinn des Gegenspielers zuzulassen. Bei 200 Wiederholungen trifft der Agent stets eine der beiden Entscheidungen und entscheidet sich nie dafür, den Stein in einer anderen Spalte zu platzieren. Beim empfohlenen Wert von $c_{UCT} = \sqrt2$ entscheidet er sich mit einer Wahrscheinlichkeit von 20,0 \%, den Zug zu blockieren, bei $c_{UCT} = 1.7$ nur 5,5 \% und bei $c_{UCT} = 1.1$ sind es 44,0 \%. Je schwächer der Exploitation-Teil der UCT-Formel gewichtet wird, desto wahrscheinlicher wird es, dass der Agent den unmittelbaren Gewinn des Gegenspielers verhindert.

Eine mögliche Erklärung für dieses Verhalten besteht darin, dass in den vom MCTS-Agenten durchgeführten Simulationen alle Züge rein zufällig gewählt werden. So kann es passieren, dass für die Züge, die den Gegenspieler nicht blockieren, selten oder gar nicht der Fall eintritt, dass er unmittelbar darauf das Spiel für sich entscheidet. Gleichzeitig besitzt der Zug, der den dritten Stein in die Kette des MCTS-Agenten platziert, eine erhöhte Gewinnwahrscheinlichkeit. So steigt der UCT-Wert des entsprechenden Knotens im MCTS-Baum über den Exploitation-Teil der UCT-Formel, was dazu führt, dass dieser Zug in den Simulationen häufiger untersucht wird und am Ende mit einer höheren Wahrscheinlichkeit gewählt wird.

Bei einem geringeren Wert für $c_{UCT}$ werden Knoten, die selten untersucht wurden, häufiger untersucht als Knoten, bei denen aus den bisherigen Simulationen eine höhere Gewinnwahrscheinlichkeit hervorgegangen ist, sodass für nicht-blockierende Züge häufiger der Fall eintritt, dass der Gegenspieler das Spiel im nächsten Zug gewinnt, wodurch sich die Gewinnrate und damit der Exploitation-Teil aller Züge, die nicht blockieren, sinkt, der blockierende Zug häufiger simuliert und damit letztendlich auch gewählt wird.

Eine mögliche Lösung zum Beheben dieser Schwachstelle besteht darin, während der Simulationen nicht alle Züge rein zufällig zu wählen, sondern die Wahrscheinlichkeit zu erhöhen, dass der Gegenspieler den vierten Stein in eine bereits existierende Dreierkette platziert, sofern dies möglich ist, sodass in den Simulationen nicht-blockierende Züge eine niedrigere Gewinnrate aufweisen und mit einer niedrigeren Wahrscheinlichkeit gewählt werden.

Da das Ziel dieser Arbeit nicht darin besteht, optimale Agenten zu entwickeln, und die Ergebnisse möglichst unabhängig von Optimierungen auf Vier Gewinnt sein sollen, wird diese Funktionalität nicht eingebaut. Aufgrund der zeitlichen Beschränkung dieser Arbeit können mögliche Nebenwirkungen von niedrigen Werten für $c_{UCT}$ nicht untersucht werden. Aus diesem Grund werden die anstehenden Messungen weiterhin mit dem Standardwert von $c_{UCT} = \sqrt2$ durchgeführt.

\subsection{PPO-Agent}

\subsection{Szenarien zur Untersuchung von Robustheit}

Um die im Konzept genannten Szenarien zu implementieren, wurde die Messumgebung um eine Klasse \texttt{DistortionGenerator} erweitert. Über dessen Konstruktor können zwei Attribute gesetzt werden, eines um die Wahrscheinlichkeit festzulegen, mit der eine Aktion verfälscht werden soll, und ein Weiteres um die Anzahl von falsch zu beobachtenden Feldern zu konfigurieren.

Sie besitzt eine Methode \texttt{distort\char`_action(orinal\char`_action: int, action\char`_mask: list[int]) -> int}, die eine von einem Agenten gewählte Aktion und die im aktuellen Zustand möglichen Aktionen entgegennimmt, und mit der im Konstruktor gegebenen Wahrscheinlichkeit eine zufällige Aktion und ansonsten die ursprüngliche Aktion zurückgibt. Sie wird immer dann ausgeführt, nachdem die Messumgebung für einen Agenten einen Zug berechnet hat und es wird die Aktion ausgeführt, die durch diese Funktion zurückgegeben wird.

Die Methode \texttt{distort\char`_state(state: numpy.ndarray(6, 7, 2)) -> numpy.ndarray(6, 7, 2)} nimmt den aktuellen Zustand des Spielfelds entgegen. Sie verwaltet zwei Listen, die jeweils Koordinaten von Spielfeldern enthalten. Eine Liste für Felder, bei denen Steiner platziert, und eine weitere Liste für Felder, bei denen Steine entfernt werden können, sodass jeweils illegale Zustände entstehen. In die erste Liste werden die Felder hinzugefügt, dessen unteren Nachbarn frei sind, und in die zweite Liste werden die Felder hinzugefügt, dessen obere Nachbarn besetzt sind. Wie in Kapitel \ref{konzept} erwähnt, wird dabei beachtet, dass aus vollen Spalten keine Spielsteine entfernt werden und keine Spalten mit nur einem freien Feld gefüllt werden, sodass sich die im beobachteten Zustand möglichen Aktionen nicht von den im tatsächlichen Zustand möglichen Aktionen unterscheiden. Die Methode wird nach jedem Spielzug durchgeführt, sobald der neue Zustand des Spielfelds bekannt ist. Das Ergebnis wird an die Agenten weitergegeben, die auf dessen Grundlage den nächsten Zug wählen.

Eine Frage, die sich bei der Implementierung des Szenarios Unsicherheit bezüglich Beobachtungen stellt, ist wie der MCTS-Agent auf Grundlage des illegalen Spielfeldzustands Simulationen durchführt. Die PettingZoo-Umgebung, die in dieser Implementierung eingesetzt wird, handhabt den Fall, dass ein Stein in eine Spalte platziert wird, in der sich ein Stein über einem freien Feld befindet, so, dass der zu platzierende Stein im untersten freien Feld landet. Eine alternative Herangehensweise wäre, den zu platzierenden Stein immer auf das freie Feld über das höchste besetzte Feld zu setzen. Es wird angenommen, dass die beiden Herangehensweisen keinen wesentlichen Unterschied in der Leistungsfähigkeit des MCTS-Agenten verursachen, daher wird das Verhalten so belassen, wie es in der PettingZoo-Umgebung umgesetzt ist.

Es war jedoch eine Anpassung an das Action Masking der PettingZoo-Umgebung notwendig, also der Art und Weise, wie in berechnet wird, welche Aktionen möglich sind. Diese Berechnung ist so implementiert, dass Spalten als nicht bespielbar gekennzeichnet werden, sobald das oberste Feld der Spalte belegt ist. Das bedeutet, wenn fälschlicherweise beobachtet wird, dass das oberste Feld einer Spalte besetzt ist, wird sie als nicht bespieltbar gekennzeichnet, auch wenn darunter freie Felder beobachtet werden. Das Action Masking wurde daher so angepasst, dass Spalten erst dann als nicht bespielbar gekennzeichnet werden, wenn alle Felder in der Spalte besetzt sind.


% Es kommen Reinforcement Learning Modelle zum Einsatz, die aus RL-Bibliotheken wie CleanRL oder Stable-Baselines bereitgestellt werden. Falls vorhanden, wird auf fertig implementierte Algorithmen zurückgegriffen.

\newpage