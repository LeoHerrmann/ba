% Allgemeine Literatur

@book{Ferguson.January2019,
	author = {Ferguson, Kevin and Pumperla, Max},
	year = {January 2019},
	title = {Deep Learning and the Game of Go},
	publisher = {{Manning Publications}}
}

@book{Heineman.October2008,
	author = {Heineman, George T. and Pollice, Gary and Selkow, Stanley},
	year = {October 2008},
	title = {Algorithms in a Nutshell},
	publisher = {{O'Reilly Media, Inc.}}
}

@book{Bewersdorff.2018,
	author       = {Jörg Bewersdorff},
	title        = {Glück, Logik und Bluff: Mathematik im Spiel - Methoden, Ergebnisse und Grenzen},
	edition      = {7},
	publisher    = {{Springer Spektrum Wiesbaden}},
	year         = {2018},
	doi          = {10.1007/978-3-658-21765-5},
	isbn         = {978-3-658-21764-8},
	date         = {2018-05-08}
}

@book{Russell.2020,
	title={Artificial Intelligence: A Modern Approach},
	author={Russell, S.J. and Russell, S. and Norvig, P.},
	isbn={9780134610993},
	lccn={2019047498},
	series={Pearson series in artificial intelligence},
	year={2020},
	publisher={Pearson}
}

@book{Sutton.2018,
	author = {Sutton, Richard S. and Barto, Andrew G.},
	title = {Reinforcement Learning: An Introduction},
	year = {2018},
	isbn = {0262039249},
	publisher = {A Bradford Book},
	address = {Cambridge, MA, USA}
}

@book{Humm.2020,
	title={Applied Artificial Intelligence: An Engineering Approach},
	author={Humm, B.G.},
	isbn={9798635591154},
	year={2020},
	publisher={Independently Published}
}

@book{Fergus.2022,
	title={Applied Deep Learning: Tools, Techniques, and Implementation},
	author={Fergus, P. and Chalmers, C.},
	isbn={9783031044199},
	series={Computational Intelligence Methods and Applications},
	url={https://books.google.de/books?id=eJv5zgEACAAJ},
	year={2022},
	publisher={Springer International Publishing}
}

@book{Früh.2022,
	title = {Foundations of Artificial Intelligence and Machine Learning},
	author = {Früh, Alfred and Haux, Dario},
	year = {2022},
	series = {Weizenbaum Series},
	pages = {25},
	volume = {29},
	address = {Berlin},
	publisher = {Weizenbaum Institute for the Networked Society - The German Internet Institute},
	issn = {2748-5587},
	doi = {https://doi.org/10.34669/WI.WS/29}
}

@book{Albrecht.2024,
	author = {Stefano V. Albrecht and Filippos Christianos and Lukas Sch\"afer},
	title = {Multi-Agent Reinforcement Learning: Foundations and Modern Approaches},
	publisher = {MIT Press},
	year = {2024},
	url = {https://www.marl-book.com}
}

@book{Buduma.2022,
	title={Fundamentals of Deep Learning: Designing Next-Generation Machine Intelligence Algorithms},
	author={Buduma, N. and Buduma, N. and Papa, J.},
	isbn={9781492082132},
	year={2022},
	publisher={O'Reilly Media}
}

% Komplexität

@article{Schaeffer.2007,
	author = {Schaeffer, Jonathan and Björnsson, Yngvi and Kishimoto, Akihiro and Müller, Martin and Lake, Robert and Lu, Paul and Sutphen, Steve},
	year = {2007},
	month = {10},
	pages = {1518-1522},
	title = {Checkers Is Solved},
	volume = {317},
	journal = {Science},
	doi = {10.1126/science.1144079}
}

@inproceedings{Allis.1994,
	title={Searching for solutions in games and artificial intelligence},
	author={Allis, Victor},
	year={1994},
	url={https://api.semanticscholar.org/CorpusID:60886521}
}

@article{Paul.2009,
	author       = {Paul, Aditya Jyoti},
	title        = {Randomized fast no-loss expert system to play tic tac toe like a human},
	journal      = {CoRR},
	volume       = {abs/2009.11225},
	year         = {2020},
	url          = {https://arxiv.org/abs/2009.11225},
	eprinttype   = {arXiv},
	eprint       = {2009.11225},
	timestamp    = {Thu, 14 Oct 2021 09:17:57 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2009-11225.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

% Vier Gewinnt

@misc{MiltonBradleyCompany.1990,
	author = "Milton Bradley Company",
	title = "Connect Four",
	year = "1990",
	howpublished = "\url{https://www.unco.edu/hewit/pdf/giant-map/connect-4-instructions.pdf}",
	note = "[Letzer Zugriff am 17. Dezember 2024]"
}

% Erste Lösungen für Vier Gewinnt

@article{Allis.1988,
	title={A Knowledge-Based Approach of Connect-Four},
	author={Allis, Victor},
	journal={J. Int. Comput. Games Assoc.},
	year={1988},
	volume={11},
	pages={165},
	url={https://api.semanticscholar.org/CorpusID:24540039}
}

@book{Allen.2010,
	author = {Allen, James Dow},
	year = {2010},
	title = {The complete book of Connect 4: history, strategy, puzzles},
	publisher = {{New York, NY : Puzzle Wright Press}}
}

@misc{Tromp,
	author = "Tromp, John",
	title = "John's Connect Four Playground",
	year = "",
	howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Wine&oldid=1262619132}",
	note = "[Letzer Zugriff am 13. Dezember 2024]"
}

% Neuere Lösungen für Vier Gewinnt

@techreport{Alderton.2019,
	author      = {Alderton, E. and Wopat, E. and Koffman, J.},
	title       = {Reinforcement Learning for Connect Four},
	institution = {Stanford University, Stanford, {California 94305, USA}},
	year        = {2019}
}

@techreport{Thill.2012,
	author      = {Thill, Markus and Koch, Patrick and Konen, Wolfgang},
	title       = {Reinforcement Learning with N-tuples on the Game Connect-4},
	institution = {Department of Computer Science, Cologne University of Applied Sciences, 51643 {Gummersbach, Germany}},
	year        = {2012}
}

@misc{Wäldchen.2022,
	title={Training Characteristic Functions with Reinforcement Learning: XAI-methods play Connect Four}, 
	author={Wäldchen, Stephan and Huber, Felix  and Pokutta, Sebastian},
	year={2022},
	eprint={2202.11797},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2202.11797}, 
}

@misc{Taylor.2024,
	title={An Evolutionary Framework for Connect-4 as Test-Bed for Comparison of Advanced Minimax, Q-Learning and MCTS}, 
	author={Taylor, Henry and Stella, Leonardo},
	year={2024},
	eprint={2405.16595},
	archivePrefix={arXiv},
	primaryClass={cs.AI},
	url={https://arxiv.org/abs/2405.16595}, 
}

@article{Sheoran.2022,
	author  = {Sheoran, Kavita and Dhand, Geetika and Dabaszs, Mayank and Dahiya,  Nishthavan and Pushparaj, Pratish},
	title   = {Solving Connect 4 Using Optimized Minimax and Monte Carlo Tree Search},
	journal = {Advances and Applications in Mathematical Sciences},
	year    = {2022},
	volume  = {21},
	number  = {6},
	pages   = {3303--3313}
}

@inproceedings{Dabas.2022,
	author="Dabas, Mayank
	and Dahiya, Nishthavan
	and Pushparaj, Pratish",
	editor="Khanna, Ashish
	and Gupta, Deepak
	and Bhattacharyya, Siddhartha
	and Hassanien, Aboul Ella
	and Anand, Sameer
	and Jaiswal, Ajay",
	title="Solving Connect 4 Using Artificial Intelligence",
	booktitle="International Conference on Innovative Computing and Communications",
	year="2022",
	publisher="Springer Singapore",
	address="Singapore",
	pages="727--735",
	isbn="978-981-16-2594-7"
}

@inproceedings{Qiu.2022,
	author={Qiu, Yiran and Wang, Zihong and Xu, Duo},
	booktitle={MEMAT 2022; 2nd International Conference on Mechanical Engineering, Intelligent Manufacturing and Automation Technology}, 
	title={Comparison of Four AI Algorithms in Connect Four}, 
	year={2022},
	volume={},
	number={},
	pages={1-5},
	keywords={},
	doi={}
}

% MCTS

@article{Browne.2012,
	author={Browne, Cameron B. and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M. and Cowling, Peter I. and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
	journal={IEEE Transactions on Computational Intelligence and AI in Games}, 
	title={A Survey of Monte Carlo Tree Search Methods}, 
	year={2012},
	volume={4},
	number={1},
	pages={1-43},
	keywords={Games;Monte Carlo methods;Artificial intelligence;Game theory;Computers;Markov processes;Decision theory;Artificial intelligence (AI);bandit-based methods;computer Go;game search;Monte Carlo tree search (MCTS);upper confidence bounds (UCB);upper confidence bounds for trees (UCT)},
	doi={10.1109/TCIAIG.2012.2186810}
}

@article{Swiechowski.2021,
	author       = {Maciej Swiechowski and
	Konrad Godlewski and
	Bartosz Sawicki and
	Jacek Mandziuk},
	title        = {Monte Carlo Tree Search: {A} Review of Recent Modifications and Applications},
	journal      = {CoRR},
	volume       = {abs/2103.04931},
	year         = {2021},
	url          = {https://arxiv.org/abs/2103.04931},
	eprinttype   = {arXiv},
	eprint       = {2103.04931},
	timestamp    = {Thu, 14 Oct 2021 09:14:46 +0200},
}

% Robustheit

@article{IEEE.1990,
	author={},
	journal={IEEE Std 610.12-1990}, 
	title={IEEE Standard Glossary of Software Engineering Terminology}, 
	year={1990},
	volume={},
	number={},
	pages={1-84},
	keywords={Terminology;Software engineering;Standards;glossary;terminology;dictionary;Software engineering;Definitions},
	doi={10.1109/IEEESTD.1990.101064}
}

@article{Moos.2022,
	AUTHOR = {Moos, Janosch and Hansel, Kay and Abdulsamad, Hany and Stark, Svenja and Clever, Debora and Peters, Jan},
	TITLE = {Robust Reinforcement Learning: A Review of Foundations and Recent Advances},
	JOURNAL = {Machine Learning and Knowledge Extraction},
	VOLUME = {4},
	YEAR = {2022},
	NUMBER = {1},
	PAGES = {276--315},
	URL = {https://www.mdpi.com/2504-4990/4/1/13},
	ISSN = {2504-4990},
	ABSTRACT = {Reinforcement learning (RL) has become a highly successful framework for learning in Markov decision processes (MDP). Due to the adoption of RL in realistic and complex environments, solution robustness becomes an increasingly important aspect of RL deployment. Nevertheless, current RL algorithms struggle with robustness to uncertainty, disturbances, or structural changes in the environment. We survey the literature on robust approaches to reinforcement learning and categorize these methods in four different ways: (i) Transition robust designs account for uncertainties in the system dynamics by manipulating the transition probabilities between states; (ii) Disturbance robust designs leverage external forces to model uncertainty in the system behavior; (iii) Action robust designs redirect transitions of the system by corrupting an agent’s output; (iv) Observation robust designs exploit or distort the perceived system state of the policy. Each of these robust designs alters a different aspect of the MDP. Additionally, we address the connection of robustness to the risk-based and entropy-regularized RL formulations. The resulting survey covers all fundamental concepts underlying the approaches to robust reinforcement learning and their recent advances.},
	DOI = {10.3390/make4010013}
}

@article{Ni.2021,
	author       = {Tianwei Ni and
	Benjamin Eysenbach and
	Ruslan Salakhutdinov},
	title        = {Recurrent Model-Free {RL} is a Strong Baseline for Many POMDPs},
	journal      = {CoRR},
	volume       = {abs/2110.05038},
	year         = {2021},
	url          = {https://arxiv.org/abs/2110.05038},
	eprinttype    = {arXiv},
	eprint       = {2110.05038},
	timestamp    = {Thu, 21 Oct 2021 16:20:08 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2110-05038.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

% Andere

@article{Garnelo.2019,
	title = {Reconciling deep learning with symbolic artificial intelligence: representing objects and relations},
	journal = {Current Opinion in Behavioral Sciences},
	volume = {29},
	pages = {17-23},
	year = {2019},
	note = {Artificial Intelligence},
	issn = {2352-1546},
	doi = {https://doi.org/10.1016/j.cobeha.2018.12.010},
	url = {https://www.sciencedirect.com/science/article/pii/S2352154618301943},
	author = {Marta Garnelo and Murray Shanahan},
}

@misc{Choromanska.2015,
	title={The Loss Surfaces of Multilayer Networks}, 
	author={Anna Choromanska and Mikael Henaff and Michael Mathieu and Gérard Ben Arous and Yann LeCun},
	year={2015},
	eprint={1412.0233},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/1412.0233}, 
}
