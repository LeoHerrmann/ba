\label{konzept}

In diesem Kapitel wird erläutert, wie im Rahmen dieser Arbeit die Fragestellung der Arbeit beantwortet werden soll, inwiefern symbolische Algorithmen oder RL-Verfahren robuster sind.

Dies geschieht exemplarisch am Beispiel von Vier Gewinnt, da dieses Spiel eine kontrollierbare Umgebung darstellt, die reproduzierbare und vergleichbare Messungen ermöglicht. Außerdem wurde in verschiedenen Studien gezeigt, dass sich sowohl symbolische als auch RL-basierte Lösungsverfahren zur Lösung dieses Spiels eigenen. Als symbolischer Algorithmus wird hier MCTS eingesetzt, da dieses Verfahren unabhängig von problemspezifischen Heuristiken angewandt werden kann, und im Gegensatz zum Minimax-Algorithmus und Alpha-Beta-Pruning auch bei komplexeren Problemen mit einer akzeptablen Laufzeit betrieben werden kann. RL-Verfahren werden in dieser Arbeit durch PPO vertreten. Dabei handelt es sich um eine Optimierung des Verfahrens A2C. Genau wie A2C handelt es sich um ein approximierendes, modellfreies on-policy Verfahren, welche sich wie in Kapitel \ref{reinforcement-learning} herausgearbeitet, zur Lösung von Vier Gewinnt am besten eigenen, ohne dabei auf spezielle Verfahren für Multi-Agent-Probleme zurückzugreifen. Im Vergleich zu A2C erleichtert PPO durch seine geringere Sensibilität gegenüber Änderungen an Hyperparametern die Implementierung.

Die Robustheit der Verfahren wird bewertet, indem Agenten, die diese Verfahren implementieren, dazu veranlasst werden, das Spiel wiederholt gegen einen zufällig spielenden Agenten zu spielen. Dabei werden die Agenten unter bestimmten Szenarien beeinträchtigt, welche auf die in Kapitel \ref{} genannten Aspekte von Robustheit abzielen und es wird gemessen und verglichen, wie stark sich die Beeinträchtigungen auf die Gewinnrate auswirken. Es wäre auch denkbar, den MCTS-Agenten und den PPO-Agenten im Spiel gegeneinander zu untersuchen, während einer der beiden Agenten oder beide Agenten den Beeinträchtigungen ausgesetzt sind. Dies hat allerdings den Nachteil, dass im Anschluss nicht klar herausgearbeitet werden kann, wie sehr ein Agent durch ein Szenario beeinträchtigt wird oder seine geschwächtes Leistungsvermögen durch den Gegenspieler ausgenutzt wird. Um die Vergleichbarkeit und Unabhängigkeit der Ergebnisse zu gewährleisten, spielen die beiden Agenten daher gegen einen zufällig spielenden Agenten.

Der Verlust der Gewinnrate eines Agenten in einem Szenario mit Beeinträchtigung wird dabei relativ zur Gewinnrate im neutralen Szenario ohne Beeinträchtigungen betrachtet, um einen Ausgleich dafür zu schaffen, dass die Agenten im neutralen Szenario nicht dieselbe Gewinnrate gegen den zufällig spielenden Agenten erzielen und die Gewinnrate eines Agenten, der im neutralen Szenario eine niedrigere Gewinnrate erzielt, auch weniger weit fallen kann. Die Betrachtung erfolgt außerdem relativ zur Gewinnrate von 50 \%, da ein Agent ab dem Punkt, ab dem er eine Gewinnrate von nur 50 \% gegen einen zufällig spielenden Agenten erzielt, keinen strategischen Vorteil mehr besitzt ist.

Daraus ergibt sich folgende Formel zur Bewertung der Robustheit:

\[l = (r_n - r_s) / (r_n - 0.5)\]

Dabei bezeichnet $r_n$ die Gewinnrate des untersuchten Agenten im Spiel gegen einen zufällig spielenden Agenten im neutralen Szenario und $r_s$ die Gewinnrate des untersuchten Agenten im betrachteten Szenario mit Beeinträchtigung. Erzielt ein untersuchter Agent im neutralen und betrachteten Szenario dieselbe Gewinnrate, wird l 0. Erzielt ein untersuchter Agent im betrachteten Szenario eine Gewinnrate von 50 \%, so ist l 1. Dazwischen verhält sich l linear zu $r_s$. Somit ist ein untersuchtes Verfahren im betrachteten Szenario robuster, je kleiner der Wert von l ist. Je größer $r_n$ ist, desto präziser lässt sich l berechnen.

Folgende Szenarien werden betrachtet:

\begin{itemize}
	\item Unsicherheit bezüglich Aktionen: Die Aktionen der Agenten bestehen darin, dass sie den nächsten Spielstein in eine freie Spalte des Spielfelds hineinwerfen, die sie auf Grundlage des beobachteten Spielfeldzustands auswählen. Dieses Szenario führt Unsicherheit bezüglich Aktionen ein, indem für einen Agenten der Spielstein mit einer bestimmten Wahrscheinlichkeit nicht in die ausgewählte Spalte, sondern in eine zufällige freie Spalte fällt.
	
	\item Unsicherheit bezüglich Beobachtungen: Die Agenten wählen zwischen möglichen Aktionen basierend auf deren Beobachtungen des Spielfelds. In diesem Szenario erhalten die Agenten fehlerhafte Informationen über das Spielfeld. Jedes Feld besitzt dabei eine bestimmte Wahrscheinlichkeit mit der nicht dessen tatsächlicher Zustand (leer, besetzt durch Spieler 1, besetzt durch Spieler 2) erkannt wird, sondern ein zufälliger Zustand.
	
	Es ist anzumerken, dass hierbei aus dem MDP ein Partially Observable MDP (POMDP) wird, also ein MDP dessen Umgebung nur teilweise oder fehlerhaft beobachtbar ist. Der betroffene Agent kann nicht mit Sicherheit gesagt werden, in welchem Zustand er sich gerade befindet. Zusätzlich zum MDP enthält das POMDP ein Observation Modell O(s, o), das die Wahrscheinlichkeit beschreibt, eine Beobachtung o im Zustand s zu machen. POMDPs sind komplizierter gezielt zu lösen, in der realen Welt jedoch wesentlich häufiger anzutreffen. Es gibt Optimierungen von MCTS und bestimmte RL-Methoden, über die POMDPs gezielt gelöst werden können, zum Beispiel indem für eine Entscheidung nicht nur der aktuelle Zustand, sondern die Historie der Zustände betrachtet wird (\cite{Russell.2020}, S. 588 ff.). Da es in dieser Arbeit darum geht, zu untersuchen, inwiefern grundsätzliche Eigenschaften von symbolischen Algorithmen und Reinforcement Learning Robustheit beeinflussen, werden diese gezielten Lösungen zur Vereinfachung nicht betrachtet. Beide Agenten gehen davon aus, dass es sich bei dem fehlerhaften Bild um den tatsächlichen Zustand des Spiels handelt, auch wenn der beobachtete Zustand gemäß der Spielregeln nicht erreicht werden könnte.
	
	\item Unsicherheit bezüglich Dynamik der Umgebung: Die Agenten erwarten ein bestimmtes Verhalten von der Umgebung, das durch die Spielregeln abgebildet wird. Der MCTS-Agent führt Simulationen anhand dieser Erwartungen aus und der RL-Agent wird auf Grundlage dieser Erwartungen trainiert. In diesem Szenario werden die Erwartungen an das Verhalten der Umgebung gebrochen, indem ein bestimmter Spieler mit einer bestimmten Wahrscheinlichkeit zwei Züge hintereinander durchführt.
	
	% \item Unsicherheit bezüglich Dynamik der Umgebung: Mit einer bestimmten Wahrscheinlichkeit führt der Gegenspieler nicht den Zug aus, den er für am besten hält, sondern einen zufälligen Zug. (Wurde rausgenommen, weil das quasi das gleiche ist wie Unsicherheit bezüglich Aktionen) nur von der anderen Perspektive.
\end{itemize}

% In den verschiedenen Szenarien gelten die veränderten Bedingungen nur für jeweils einen Agenten. Wenn beide Agenten gleichzeitig von den veränderten Bedingungen betroffen wären, könnten nur Aussagen über die relative Robustheit zueinander getroffen werden. Dadurch dass die veränderten Bedingungen nur für jeweils ein Verfahren auf einmal gelten, kann genau gesagt werden, welches Verfahren wie stark betroffen ist.

% Die Messungen werden nicht nur unter Szenarien mit veränderten Bedingungen durchgeführt, sondern auch in einer neutralen Umgebung ohne veränderte Bedingungen, um eine Grundlage für die Analyse der Ergebnisse zu bilden.

% Um einen fairen Vergleich zu gewährleisten, müssen die Agenten in der neutralen Umgebung gleich stark sein. Dazu werden die Parameter der Agenten (Anzahl der Simulationen beim MCTS Agenten, Anzahl der durchlaufenen Self-Play-Trainingsepisoden beim RL-Agenten) so eingestellt, dass sie in der neutralen Umgebung im Spiel gegeneinander jeweils eine Gewinnrate von etwa 50\% aufweisen.

Da bei Vier Gewinnt, wie in Kapitel \ref{vier-gewinnt} erwähnt, der Spieler, der den ersten Stein platziert, einen Vorteil hat, wechselt bei den Messungen, sofern nicht anders gekennzeichnet, zu Beginn jedes Spiels das Recht, den ersten Zug zu durchzuführen.

Es ist zu beachten, dass die Agenten auf einem ähnlichen und gewissermaßen starkem Level spielen. Denn bei Agenten, die ohnehin nicht stark spielen, wird es schwierig sein, in den verschiedenen Szenarien zur Untersuchung der Robustheit eine aussagekräftige Änderung in den Gewinnraten zu messen. Außerdem ist anzunehmen, dass ein stärkerer Agent Entscheidungen trifft, die längerfristig zum Erfolg führen, sodass er auch die Beeinträchtigungen in den verschiedenen Szenarien besser wegstecken kann als ein schwächerer Agent. Als Indikator für die Spielstärke der Agenten wird die durchschnittliche Gewinnrate und Spieldauer im Spiel gegen zufällig spielende Agenten bei abwechselndem Anzugsrecht und im Spiel gegen sich selbst bei konstantem Anzugsrecht verwendet. Im Spiel gegen zufällig spielende Agenten sollten starke Agenten hohe Gewinnraten bei kurzer Spieldauer erzielen. Im Spiel gegen sich selbst sollte der erste Spieler aufgrund des Vorteils des Anzugsrechts eine hohe Gewinnrate erzielen und auch die Spieldauer wird sich verlängern, da der Gegenspieler auch besser verteidigt, sodass komplexere Spielsituationen entstehen und sich die Entscheidung des Spiels hinauszögert. Zur Beurteilung der Spielstärke der Agenten erfolgt außerdem eine kurze qualitative Analyse im Spiel gegen einen Menschen.
