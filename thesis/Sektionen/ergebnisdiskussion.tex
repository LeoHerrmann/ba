Um die Robustheit der im Rahmen dieser Arbeit implementierten MCTS- und PPO-Agenten miteinander zu vergleichen, wurden sie in den in Kapitel \ref{robustheit-szenarien} beschriebenen Szenarien im Spiel gegen einen zufällig spielenden Agenten evaluiert. Die Agenten spielen dabei mit abwechselndem Anzugsrecht gegen einen zufällig spielenden Agenten. Gemessen wird die Gewinnrate der zu untersuchenden Agenten in Abhängigkeit des Ausmaßes der in den Szenarien eingeführten Unsicherheit.

Für jeden untersuchten Unsicherheitsgrad wurden mit dem PPO-Agenten 2000 Spiele durchgeführt. Bei den Untersuchungen mit dem MCTS-Agenten wurden auf Grund des erhöhten Rechenbedarfs und der begrenzten Zeit nur 200 Spiele durchgeführt. Wie in Kapitel \ref{konzept} beschrieben, wird der Gewinnratenverlust als relatives Maß für die Robustheit der Verfahren verwendet. Ein höherer Gewinnratenverlust deutet dabei auf ein weniger robustes Verfahren hin. Die Diagramme zu den tatsächlich gemessenen Gewinnraten im Spiel gegen den zufällig spielenden Agenten befinden sich im Anhang unter \ref{appendix-original-win-rates}.

\subsection{Unsicherheit bezüglich Aktionen}

\label{uncertain-actions-result}

Beim Vergleich der Verfahren hinsichtlich der Robustheit gegenüber Unsicherheiten bezüglich Aktionen wurde die Wahrscheinlichkeit, anstelle des durch den Agenten gewählten Zuges einen zufälligen Zug durchzuführen, in 10-Prozentpunktschritten von 0 \% bis 100 \% variiert.

\begin{figure}[ht!]%[!tbp]
	\includegraphics[width=0.7\textwidth, center]{Bilder/robustness-results/uncertain_actions_win_rate_losses.png}
	\caption{Gewinnratenverlust in Abhängigkeit von der Wahrscheinlichkeit, eine zufällige Aktion durchzuführen.}
\end{figure}

Beträgt diese Wahrscheinlichkeit 0 \%, befindet sich der Gewinnratenverlust bei beiden Agenten ebenfalls bei 0 \%, da es sich bei den dabei erzielten Gewinnraten um den Ausgangswert für die Berechnung des Gewinnratenverlustes handelt.

Ab einer Wahrscheinlichkeit für zufällige Aktionen von 20 \% ist für den MCTS-Agenten ein signifikant niedrigerer Verlust von 1,0 \% (95 \%-CI: 0,2 - 5,6) gegenüber von 11,3 \% (95 \%-CI: 9,0 - 14,0) bei PPO zu verzeichnen. Einen solche signifikante Differenz hält sich bis zu einer Wahrscheinlichkeit von 80 \% für zufällige Aktionen, wo ein Gewinnratenverlust von 54 \% (95 \%-CI: 42,6 - 67,0) beim MCTS-Agenten und 78,7 \% (95 \%-CI: 74,2 - 83,3) beim PPO-Agenten ermittelt wurde. Damit ist der MCTS-Agent in diesem Bereich signifikant robuster als der PPO-Agent.

Bei 90 \% liegen die Messpunkte von MCTS mit 66,0 \% (95 \%-CI: 53,8 - 79,6) gegenüber 85,4 \% (95 \%-CI: 80,8 - 90,1) zwar weiterhin höher, jedoch liegen die Konfidenzintervalle der Messpunkte so nah bei einander, dass aus den Werten allein nicht mit ausreichender Gewissheit Schlüsse über die tatsächlichen Werte gezogen werden können.

Bei einer Wahrscheinlichkeit für zufällige Aktionen von 100 \% sollten beide Agenten nach genügend durchgeführten Spielen gegen den zufällig spielenden Agenten eine Gewinnrate von 50 \% und damit einen Gewinnratenverlust von 100 \% erzielen. Die Messpunkte liegen für MCTS mit 112,0 \% (95 \%-CI: 98,2 - 125,4) leicht darüber bzw. für PPO mit 99,2 \% (95 \%-CI: 94,5 - 108,3) leicht darunter. Da ein Gewinnratenverlust von 100 \% innerhalb der Konfidenzintervalle liegt, ist es naheliegend, dass die gemessenen Abweichungen stochastisch bedingt sind.

Aus den Messungen geht für eine Wahrscheinlichkeit für zufällige Aktionen von 20 \% bis 80 \% der MCTS-Agent als der robustere Agent hervor. Für darunter bzw. darüber liegende Wahrscheinlichkeiten kann aufgrund von Messungenauigkeiten kein signifikanter Unterschied gemessen werden. Der Verlauf des Gewinnratenverlustes liegt jedoch nahe, dass MCTS auch dort weiterhin einen niedrigen tatsächlichen Gewinnratenverlust aufweist.

Es wurde nicht erwartet, im Szenario mit Unsicherheiten bezüglich Aktionen eine signifikante Differenz im Gewinnratenverlust zwischen den beiden Verfahren zu sehen, die durch die Verfahren selbst bedingt ist. Es liegt nahe, dass die Diskrepanz dadurch hervorgerufen wird, dass der PPO-Agent unzureichend trainiert wurde, wodurch er eine kurzfristigere Strategie besitzt, die anfälliger für Störungen ist.

\subsection{Unsicherheit bezüglich Beobachtungen}

Beim Vergleich der Verfahren hinsichtlich der Robustheit gegenüber Unsicherheiten bezüglich Beobachtungen wurde die Anzahl von veränderten Spielsteinen, in 2er-Schritten von 0 bis 20 variiert. Aufgrund der größer werdenden Konfidenzintervalle wurden ab 10 veränderten Spielsteinen nicht mit 200 sondern 500 Spiele zwischen dem MCTS-Agenten und dem zufällig spielenden Agenten ausgetragen.

\begin{figure}[ht!]%[!tbp]
	\includegraphics[width=0.7\textwidth, center]{Bilder/robustness-results/uncertain_observations_win_rate_losses.png}
	\caption{Gewinnratenverlust in Abhängigkeit von der Anzahl der veränderten Spielsteine.}
\end{figure}

Ähnlich wie im vorherigen Abschnitt \ref{uncertain-actions-result} startet der Gewinnratenverlust beider Agenten bei 0 veränderten Spielsteinen mit 0 \%.

Ab 4 veränderten Spielsteinen lässt sich beim MCTS-Agenten mit 3,0 \% (95 \%-CI: 1,0 - 8,6) ein signifikant niedrigerer Gewinnratenverlust gegenüber des PPO-Agenten mit 11,7 \% (95 \%-CI: 9,4 - 14,5) verzeichnen. Diese Diskrepanz hält sich bis zu einer Anzahl von 10 veränderten Spielsteinen, wobei der MCTS-Agent einen Gewinnratenverlust von 14,0 \% (95 \%-CI: 10,2 - 19,2) und der PPO-Agent einen Verlust von 24,9 \% (95 \%-CI: 21,9 - 28,4) erzielt. Das zeigt, dass sich der MCTS-Agent in diesem Bereich signifikant robuster verhält als der PPO-Agent.

Ab 12 veränderten Spielsteinen sind die Differenzen zwischen den Messpunkten zu gering und die Konfidenzintervalle zu groß, dass nicht mit ausreichender Sicherheit bestimmt werden kann, bei welchem Agenten die tatsächlichen Werte höher oder niedriger liegen.

Bei 18 und 20 veränderten Spielsteinen liegt der gemessene Verlust von MCTS mit 42,4 \% (95 \%-CI: 35,6 - 50,0) und 44,8 \% (95 \%-CI: 38,0 - 52,6) sogar über dem von PPO mit 37,0 \% (95 \%-CI: 33,5 - 40,9) in beiden Fällen. Aufgrund der großen Konfidenzintervalle kann jedoch nicht gesagt werden, ob sich die tatsächlichen Werte genauso verhalten. Um Aussagen darüber treffen zu können, sind Messungen mit mehr Wiederholungen nötig.

Bei 4 bis 10 veränderten Spielsteinen geht der MCTS-Agent als der robustere Agent hervor. Aufgrund des Verlaufs des Gewinnratenverlustes in Abhängigkeit der Anzahl der veränderten Spielsteinen liegt nahe, dass dies auch für eine Anzahl von unter 4 gilt. Liegt die Anzahl der veränderten Spielsteinen über 10, können aufgrund der zu großen Messungenauigkeiten keine signifikanten Unterschiede festgestellt werden. Die beobachtete höhere Robustheit des MCTS-Agenten könnte durch die Anfälligkeit von neuronalen Netzwerken gegenüber unerwarteten Beobachtungen verursacht worden sein. Ebenso wie in Abschnitt \ref{uncertain-actions-result} könnte die Ursache für dieses Verhalten jedoch auch in der kurzfristigen Strategie des in dieser Arbeit unzureichend trainierten PPO-Modells liegen.

\subsection{Verallgemeinerungen}

Die Messergebnisse zeigen, dass der im Rahmen dieser Arbeit implementierte MCTS-Agent robuster gegenüber Unsicherheiten bezüglich Aktionen ist als der implementierte PPO-Agent. Dies gilt auch für die Robustheit gegenüber Unsicherheiten bezüglich Beobachtungen, zumindest bis zu dem Grad, ab dem die Messungenauigkeiten den Vergleich des Gewinnratenverlustes zwischen den beiden Agenten nicht mehr zulassen.

In Kapitel \ref{training-random} wurde gezeigt, dass der implementierte PPO-Agent schlechter spielt als der MCTS-Agent, da das Training gegen einen zufällig spielenden Agenten nicht genügt, um eine langfristig ausgerichtetes Regelwerk zu erlernen. Es liegt nahe, dass die kurzfristig ausgerichtete Strategie des PPO-Agenten auch anfälliger für Störungen ist. Damit kann nicht bestimmt werden, ob die Ursache für die gemessene niedrigere Robustheit in der bereits in der neutralen Umgebung schlechteren Leistungsfähigkeit oder tatsächlich an der niedrigeren Robustheit des PPO-Verfahrens liegt. Daher gelten die Aussagen nur für die konkreten Implementierungen der Verfahren in dieser Arbeit und können nicht auf alle Implementierungen der Verfahren verallgemeinert werden. Damit sind auch keine allgemeinen Aussagen über die Robustheit von symbolischen Algorithmen und RL-Verfahren im Vergleich möglich.

Um Aussagen über das Verhalten der Agenten auf die Verfahren MCTS und PPO bzw. symbolische Algorithmen und RL-Verfahren verallgemeinern zu können, muss sichergestellt sein, dass die Agenten unter neutralen Bedingungen möglichst eine gleichwertige Leistung erzielen. Im Rahmen dieser Arbeit konnten durch quantitative und qualitative Analysen der Agenten gezeigt werden, dass dies in diesem Fall nicht zutrifft. Es ist kritisch zu hinterfragen, inwiefern diese Methodiken ausreichen würden, um zu zeigen, dass die Agenten eine gleichwertige Leistung erzielen. Denn im Spiel gegen einen zufällig spielenden Agenten spiegeln sich große Leistungsunterschiede nur in sehr geringfügigen Differenzen wieder. Und in der qualitativen Analyse können subjektive Verzerrungen der Wahrnehmung das Ergebnis beeinflussen. Besser geeignet sind beispielsweise das Spiel gegen einen perfekt spielenden Agenten wie in \cite{Thill.2012} oder gegen unterschiedliche nicht-perfekt spielende Agenten in einer Turnier-Umgebung, so wie es in \cite{Zhong.2020} umgesetzt wurde.

Es liegt nahe, dass durch die Untersuchung von zwei Verfahren an einem Anwendungsfall nicht ausreicht, um verallgemeinernde Aussagen über den Vergleich der Robustheit zwischen symbolischen Algorithmen und RL-Verfahren treffen zu können, sondern dazu verschiedene Anwendungsfälle und verschiedene Lösungsverfahren untersucht werden müssen.
